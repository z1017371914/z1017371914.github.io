本文主要介绍分布式常用的算法以及理论知识。

## 拜占庭将军问题

`拜占庭将军问题`:n个将军中通过口信来传递作战方式为进攻还是撤退，如果有将军是叛徒，那么需要方法来使得未背叛的将军的决定能消除掉叛徒将军的决定。

拜占庭将军问题，模拟的是分布式系统中，除了故障外的一种恶意场景，即存在恶意节点。

根据是否存在恶意节点的解决方法，可以分为`拜占庭容错算法`和`非拜占庭容错算法`

`拜占庭容错算法`：Byzantine Fault Tolerance（BFT）. 常见算法：`PBFT算法`，`Pow算法`

`非拜占庭容错算法`：Crash Fault Tolerance（CFT）. 常见算法：`Paxos算法`，`Raft算法`，`ZAB协议`

### 口信消息型拜占庭问题

通过增加将军的人数来解决拜占庭问题。

1.m个叛徒，将军数大于等于3m+1可保证成功。但是需要m + 1轮的商议。

2.n个将军，最多能容忍 (n - 1) / 3 个叛徒。

### 签名消息型拜占庭问题

忠诚将军利用类似于信物一样的东西来表示决定，任何将军都可以验证信物是否被修改。

开始时忠诚的将军会给其他所有人发送作战信息，如果某个将军发现收到的另一将军的方式和忠诚将军的不一致，则将其视为叛徒，将他踢出局，重新商议作战方式。

## CAP理论

1.CAP由三部分组成：

`Consistency`一致性。请求访问到不同的节点，获取一样的结果，虽是集群，却像单机的数据一样。

`Availability`可用性。可用性保证一定响应请求，但不保证数据的一致性！

`Partition Tolerance`分区容错性。某节点异常时，不影响整体对外提供服务。

2.CAP是一个`不可能三角`，无法实现三个特性都能得到满足。

当有网络分区时，就需要进行网络传输，这种情况下需要保证`P`.需要在`C`和`A`中抉择。

>`CP`:当消息丢失或延迟过高时，部分节点无法保证信息是最新的，从而系统拒绝进行写入信息，也就是拒绝提供对外服务，也就是不满足`A`
>
>`AP`:当有请求信息时，被选中的节点会交付它最新的信息，但不保证是全局的最新信息，也就是不满足`C`

以上的场景是故障出现时的取舍，但大部分情况下系统是正常运行的，所以一般是选择`CA`,只有节点故障时，才选择`P`,在`C`和`A`中取舍。

3.应用场景

`CA`：单机MySQL

`CP`：ETCD，Zookeeper

`AP`： Cassandra，DynamoDB

## ACID理论

单机应用上的ACID(即事务的特性)一般通过`锁`，`时间序列`等。

分布式应用则不一样。

为保证集群之间的ACID特性，`两阶段提交协议`和`TCC可实现`

### 两阶段提交协议

前提条件：

>1.集群分成了两部分：`协调者coordinator`，`事务参与者worker`.
>
>2.`协调者`只有一个，负责管理worker的事务提交
>
>3.`worker`有很多个，任意worker之间可通信，每个worker都有事务回滚的能力，先前的记录已经进行了持久化(写到磁盘)

实现过程：

>1.请求阶段：协调者通知worker表明自己是需要`提交事务`，还是`取消事务`.所有worker都会向协调者表明自己的决定
>
>2.提交阶段：协调者收到worker的决定，当且仅当所有的worker均为提交事务时，才会同意。否则命令所有worker进行回滚。

缺点：通过`阻塞`的完成协议。节点等待消息的时候是阻塞状态的，其他进程无法获取到CPU资源。

甚至，如果协调者宕机，那么就会永久的阻塞。

### TCC

1.`TCC`：Try-Confirm-Cancel，预留-确认-撤销

2.TCC和两阶段提交协议的区别是，TCC需要预先分配所需要的资源，从而再判断需要提交还是回滚。

3.`TCC`本质是补偿事务，针对每个操作都注册一个与其对应的`确认操作`和`补偿操作`.

4.`TCC`是`业务层面`的协议,不依赖于数据库来实现事务，而是通过业务代码来实现，减少了数据库的压力。但同时，业务代码也会很难实现。

> 有个腾讯的同学提起过他们支付业务对于2PC与TCC的取舍：2PC+定时任务撤掉提交。支付业务宁可超时也不可出错，而且TCC实现起来也很困难，所以他们选择了2PC+定时任务撤销提交。如果使用TCC，那实现场景会是这样：用户的扣款划入到冻结资金(相当于预分配资源)，再去决定是提交修改或者撤销。

## BASE理论

1.`BASE`是`AP理论`的延申，强调可用性。

> 因为如果使用CP模型，通过节点的可用性相乘能得到分布式系统的可用性.
>
> 假设 3 个节点的集群，每个节点的可用性为 99.9％，那么整个集群的可用性为 99.7％，也就是说，每个月约宕机 129.6 分钟，这是非常严重的问题。

2.BASE的两个重要模块`Basically Available`和`Eventually Consistent`，即`基本可用`与`最终一致性`.

### 实现基本可用

1.`基本可用`：当分布式系统遇到不可预知的故障时，通过降低其他服务的优先级，保证核心服务的可用性。

2.实现方式：

>①`流量削峰`：将流量请求分到不同的时间段。
>
>> 比如12306的分时间售票：9️⃣:0️⃣0️⃣深圳北➖广州南，🔟:0️⃣0️⃣北京➖上海
>
>②`延迟响应`：请求不需要马上响应，可适当提高响应时间。
>
>>比如12306流量突增时，会让你进行等待。牺牲了你的等待时间，换来了系统应对高流量的可用性。
>
>③`体验降级`：将展示的资源换成更小的。
>
>>比如某明星艳照曝光，大量用户访问其照片，造成网络的拥塞，可将照片换成小图片，降低清晰度，提升系统的处理能力。
>
>④`过载保护`：请求放入队列，超时了就直接拒绝请求。

### 实现最终一致

1.`最终一致`：数据会有短暂的时间不同步，称为`软状态`。但经过一定的时间，最终一定会实现同步，这就是`最终一致`。

2.与之对应的的`强一致性`：无法容忍任何时间的数据不一致性，如金融支付场景。

3.一致与否的标准：

>①以某一数据为基准。其他数据若不与其相等，判定为不一致。
>
>②以最新写入的数据为基准。

4.实现方式:

>①`读时修复`：在读取数据时，检测数据是否不一致，并进行修复。
>
>>比如 Cassandra 的 Read  Repair 实现，具体来说，在向 Cassandra 系统查询数据的时候，如果检测到不同节点的副本数据不一致，系统就自动修复数据.
>
>②`写时修复`：对写入数据时，检测数据的不一致，并进行修复
>
>>如 Cassandra 的 Hinted Handoff 实现。Cassandra 集群的节点之间远程写数据的时候，如果写失败,就将数据缓存下来，然后定时重传，修复数据的不一致性。
>
>③`异步修复`：通过定时任务，检测副本数据是否一致。

## PAXOS算法

1.`PAXOS`算法核心为两部分：`Basic Paxos`算法，`Multi-Paxos`思想。

2.`Paxos`算法可以让各个节点之间达成`"共识"`

### Basic Paxos

1.`Basic Paxos`用于解决多节点之间如何就某个值（提案 Value）达成共识.

>假设我们要实现一个分布式集群，这个集群是由节点 A、B、C 组成，提供只读 KV 存储服
>务。创建只读变量的时候，必须要对它进行赋值，而且这个值后续没办法修改。因此一个节点创建只读变量后就不能再修改它了，所以所有节点必须要先对只读变量的值达成共识，然后所有节点再一起创建这个只读变量。

2.`Basic Paxos`赋予节点`角色`的概念。每个节点可以是多个角色。

3.`角色`有三种:

>①`Proposer`：提议者。一般是接收到请求的节点。提议者代表的是接入和协调功能，收到客户端请求后，发起`二阶段提交`，进行共识协商。
>
>②`Accepter`：接收者。对每个提议者提议的值进行投票，并存储接受的值。一般集群所有节点都是接收者。
>
>③`Learner`：学习者。接受达成共识的值，不参与投票。一般是`数据备份节点`充当学习者。

4.**达成共识的实现**

>问题场景：存在`客户端1`，`客户端2`,以及三个节点：`节点A`，`节点B`，`节点C`。Client1的提议编号为1，Client2的提议编号为5. (:label:：Client的请求是这样的格式(提议编号，写入的值) ​)。同时A，B先收到Client1的请求，C先收到Client2的请求。
>
>**实现过程：**
>
>>1️⃣**prepare阶段**：
>>
>>①Client1和Client2都给A，B，C发送自己的`提议编号`,此时并`没有发送写入的值`！
>>
>>②A，B收到Client1的提议编号，C收到Client2的提议编号。
>>
>>③A，B给提议者回复**之前未收到提议，承诺不再响应编号小于等于1的提议编号，不会通过标号小于1的提案**；C给提议者回复**之前未收到提议，承诺不再响应编号小于等于5的提议编号，不会通过标号小于5的提案**
>>
>>![image](https://tva2.sinaimg.cn/large/0085EwgIgy1gsrbdek0lrj30pm09l0vn.jpg)
>>
>>④A，B收到Client2为5的提议，C收到Client1为1的提议
>>
>>⑤根据承诺，A，B需要响应Client2的提议，于是**回应提议方之前未收到提议，承诺不再响应编号小于等于5的提议编号，不会通过标号小于5的提案**；C根据承诺，**不会回应Client1的提议，将其进行丢弃**。
>>
>>![image](https://tvax3.sinaimg.cn/large/0085EwgIgy1gsrbdryzq9j30ol09x771.jpg)
>>
>>2️⃣**accept阶段**:
>>
>>①Client1和Client2接收到准备响应的消息，将自己的(提议编号，写入的值)发送给节点A，B，C
>>
>>![image](https://tvax4.sinaimg.cn/large/0085EwgIgy1gsrbfa0ujfj30pc099tbh.jpg)
>>
>>②A，B，C收到提案后，根据承诺来写数据。A，B，C**虽然收到Client1的数据，但会遵守承诺，丢弃其提案不执行**。**收到Client2的提案，满足自己的承诺，进行写入**。
>
>至此，完成了达成共识的目的。
>
>tips：如果集群中有`学习者`，接受者通过了提案并写入后，会通知所有学习者进行提案的写入。

5.`Basic Paxos`的容错：当故障节点少于一般时，集群仍能工作。区别于分布式事务，需要所有节点都同意，才能提交。

### Multi Paxos

1.进行多次`basic paxos`的问题：提案冲突；两次`RPC`造成时延大。

>①进行多次basic paxos，可能会出现无法出现多数票的情况：一个 5 节点的集群，如果 3 
>个节点作为提议者同时提案，就可能发生因为没有提议者接收大多数响应（比如 1 个提
>议者接收到 1 个准备响应，另外 2 个提议者分别接收到 2 个准备响应）而准备失败，需
>要重新协商
>
>②两次RPC(准备阶段+接受阶段)，往返的消息比较多，时延大。

2.`Multi Paxos`相较于`Basic Paxos`的`改进`：**引进了Leader**；**改进Basic Paxos执行过程**

![image](https://tvax2.sinaimg.cn/large/0085EwgIgy1gtard8c006j30o90engoi.jpg)

3.改进的执行过程：**当Leader处于稳定状态时，省去准备阶段，以Leader的命令为准，进入到接受阶段**。

这个过程省去了投票阶段的时延，也不会存在投票冲突。

4.写数据：和Leader交互，再由Leader下发命令来进入到接受阶段。

![image](https://tvax4.sinaimg.cn/large/0085EwgIgy1gtarg6tw8bj30o80dijtx.jpg)

5.读数据：直接返回Leader的值。

![image](https://tvax4.sinaimg.cn/large/0085EwgIgy1gtargce32xj30p50do0uh.jpg)

6.不足：读写都在Leader上进行，相当于单机性能。

## Raft算法

### leader选举

1.`Raft`是基于`Multi-Paxos`的一致性共识算法，也是一切以Leader为中心来决策的。

2.Raft的成员有三种身份：`leader`,`follower`,`candidate`

>follower：接受leader的信息并进行处理。定时和leader进行心跳交流，如果leader失联，就会将自己推荐成candidate
>
>candidate：向其他节点发送投票的信息，如果收到最多的投票，那么候选人就会成为Leader
>
>leader：处理写请求，管理日志复制，发送心跳包。

3.选举leader的过程：

>①初始时刻，所有节点均是`follower`状态,每个节点自己都有随机的心跳超时时间(超过时间仍未收到leader的心跳包，就会毛遂自荐为leader)。
>
>![image](https://tvax2.sinaimg.cn/large/0085EwgIgy1gtarrn67q9j30q60k5din.jpg)
>
>2.首次超时的candidate，在这第一次任期中，发送自荐包(带上了任期时间)给其他节点。如果其他节点在此任期内未收到其他leader的消息，就会同意该candidate的自荐。
>
>![image](https://tva2.sinaimg.cn/large/0085EwgIgy1gtaruk2ejij30mc0onq63.jpg)
>
>3.A收到大多数选票，记录自己的选票数，当选Leader。而其他节点会标记他们投票时的任期数，如图中的1。
>
>![image](https://tvax2.sinaimg.cn/large/0085EwgIgy1gtaruk2ejij30mc0onq63.jpg)
>
>4.A当选leader后，不断发送心跳包给其他节点来声明”我还活着，请不要同意其他节点的投票“
>
>![image](https://tvax3.sinaimg.cn/large/0085EwgIgy1gtarwrfg6aj30pr0nntby.jpg)

4.整个过程设计到两种RPC：`请求投票RPC`,`日志复制RPC`

>请求投票：超时节点发送请求投票的信息给其他节点。
>
>日志复制：复制日志+发送心跳包

5.任期信息的规则：

>①每个节点都会有自己的任期号。当某节点想自荐时，就会发送他自己的任期号+1的请求投票RPC给其他节点
>
>②每个节点收到比自己大的任期号信息时，会将自己的任期也改成收到所有信息中最大的任期号
>
>③如果某个节点是leader或者candidate，收到了比自己任期大的信息，那么直接降级为follower
>
>④如果某个节点收到了任期号小的请求，那么会直接拒接该请求。
>
>个人拙见：在我看来，任期像是表明这个节点经历了几个回合。它的回合数少，那就说明发生了丢包，他说的话就都不会被算数，一切以经历了最多回合的节点为准。

6.选举的规则：

>①leader周期性发送心跳信息来阻止其他节点进行选举
>
>②如果节点超时了仍未收到leader消息，就会变成candidate，就会向其他节点发送投票请求RPC
>
>③获得票数最多的节点会升级为leader
>
>④follower的投票遵循first come first vote。A收到B的任期为4的RPC，再收到C的任期为4的RPC时会以无选票为理由拒绝C的投票请求。也就是一人一票制，先到先投票。

7.如何保证选举不会出现”无多数票的情况“？

>每个节点都有随机超时时间。这样，大部分情况下都只有一个节点超时，然后自荐为leader。

### 日志复制

1.`日志`：由日志项组成。

2.`日志项`：由指令，任期数，索引组成。

![image](https://tvax1.sinaimg.cn/large/0085EwgIgy1gtasheupb7j30qg0gxdjl.jpg)

3.在Raft中，副本是以日志的形式存在的，通过日志的一致来实现共识。 而leader处理写请求，其实就是`日志复制`和`日志提交`的过程.因此，需要关注Raft的日志复制，以及如何实现日志一致。

4.日志复制的过程：

>![image](https://tvax1.sinaimg.cn/large/0085EwgIgy1gtat09qtepj30pe0bogoi.jpg)
>
>①leader接收到client的请求，将请求与转成日志项
>
>②leader通过日志复制RPC，向其他节点发送日志复制
>
>③leader成功收到大多数的日志复制成功时，将自己的日志提交到状态机中，进行实际的操作。
>
>④leader向client返回执行结果
>
>⑤其他节点提交leader的日志记录

在这个过程中，leader并不会等到follower都提交日志记录才会向client返回结果，确实也没那个必要。而是等大多数节点都成功收到日志并且复制成功时，就会提交日志记录，从而返回结果给client。

5.日志复制存在的问题：由于进程崩溃，服务器宕机等问题导致的日志丢失，会导致日志不具有一致性。

6.Raft如何解决上述说的`日志不一致性`？

 >![image](https://tva4.sinaimg.cn/large/0085EwgIgy1gtatb4p2mdj30oz0gq42o.jpg)
 >
 >1.leader发送RPC日志复制给follower时，会附上此次需要复制的日志项的`上一个索引值index`，`上一个任期数term`。
 >
 >2.follower收到日志复制RPC时，如果follower最大索引对应的日志项和index，term对应不上，那就会向leader返回失败的信息
 >
 >3.leader收到消息后向该节点发送比index要小1的日志项：index-1，term’。如此往复直到follower返回success信息。
 >
 >4.follower知道了自己丢失了哪些日志项，于是就从丢失处开始复制leader的日志项。最终实现日志一致。

### 成员变更

1.成员变更：业务需要，从而会增加或者减少节点的数量，这种情况下可能会出现集群的分裂，出现两个leader。

>配置：声明集群的节点范围。
>
>![image](https://tvax3.sinaimg.cn/large/0085EwgIgy1gtatkgfinfj30ob08kgnd.jpg)
>
>例如初始配置为（A,B,C),此时增加节点D，E。理论上配置应该为(A,B,C,D,E)。但事与愿违。
>
>![image](https://tvax2.sinaimg.cn/large/0085EwgIgy1gtatntdf5bj30o9067dj5.jpg)
>
>有可能C节点和A，B出现了网络分区(脑裂)，出现了(A，B，C)和(C,D,E)两个配置，进而出现两个leader的现象，这就是成员变更会出现的问题。

2.Raft的解决方法：`单节点变更(single-servers changes)`

>单节点变更：将添加/删除多个节点拆分成多次的添加/删除一个节点。
>
>实现过程：
>
>①需要添加节点D，leaderA就会先向节点D同步日志信息
>
>②将新的集群配置(A,B,C,D)作为日志项，发送给所有的节点。然后leader提交到自己的状态机，完成单节点的引入。
>
>同理，需要添加E时也一样，需要添加多少节点，就需要进行多少次单节点变更。
>
>单节点添加并发执行可能会出现某单节点还没执行结束，就添加新的单节点。

## 一致Hash算法

