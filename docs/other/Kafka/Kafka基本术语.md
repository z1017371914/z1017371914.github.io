## Kafka基本术语

1.`主题Topic`：Kafka中发布、订阅消息的都是主题

2.`生产者Producer`：向主题发布消息的客户端

3.`消费者Consumer`：订阅主题信息的客户端

4.`Broker`：Kafka服务端的进程。一个Kafka集群由多个Broker进程组成。

- Broker负责**接收**,**处理**客户端发送的请求，对消息进行**持久化**

5.`备份机制Replication`：Kafka使用的是 leader replica - follower replica的备份方式。不同于MySQL等，从库也能处理客户端请求，**Kafka只有leader能处理请求**。

6.`备份的实现`：follower节点向leader节点发送同步请求，leader节点向follower节点发送新消息。

7.`伸缩性Scalability`：当消息太多导致leader对应的机器的Broker进程无法容纳时，Kafka是怎么解决的呢？

- 将数据分割成很多分，保存在不同的Broker进程上，这就是所谓的`分区`

8.`Kafka的消息架构`：分成了三层：主题层，分区层，消息层

- **主题层**：每个主题分成了M个分区，每个分区有N个副本
- **分区层**：有M个分区，每个分区组成是由一个leader副本和 N - 1个follower副本组成
- **消息层**：分区中包含若干条消息，每条消息的位移从 0 开始，依次递增。

![image](https://tva4.sinaimg.cn/large/0085EwgIgy1gteiywbfmrj60nc0cu78102.jpg)

9.`Kafka持久化数据的方式`：通过日志，采用追加写的方式，来进行持久化。

- 好处：顺序的IO操作降低了时延，提高了Kafka的吞吐率
- 可能的问题：磁盘终究会满，何时删除是问题。为此对日志进行了一些操作来及时删掉不再需要的旧日志

10.`Kafka的日志`：日志不是一整个大文件，而是分成了很多`日志段`，当一个日志段写满后，就会自动且分出新的日志段，将老的日志段存起来。老日志段会被持久化到磁盘中，Kafka也会有定时任务来定期检查老的日志段是否能被删除，从而解放磁盘资源

11.`消费者组Consumer Group`：为了实现点对点的消费模型，Kafka引入了消费者组的概念。将只能互斥消费的消费者归类为消费者组，消息来临时，组内只能由一个消费者实例去消费信息

12.`重平衡Rebalance`：一个消费者组中的某个实例挂了，可以将其为消费完的消息给组内的其他某个健康的节点。

13.`消费者 Consumer Offset`：消费者消费到消息的位置指针。需要区别于前面的分区位移(这是固定的，表明了消息在分区中的位置)





----

**为什么Kafka的follower节点不对外提供服务？**

引用自[知乎](https://www.zhihu.com/question/327925275/answer/705690755)：

第一：主从分离与否没有绝对的优劣，它仅仅是一种架构设计，各自有适用的场景。

第二、如你所说，Redis和MySQL都支持主从读写分离，我个人觉得这和它们的使用场景有关。对于那种读操作很多而写操作相对不频繁的负载类型而言，采用读写分离是非常不错的方案——我们可以添加很多follower横向扩展，提升读操作性能。反观Kafka，它的主要场景还是在消息引擎而不是以数据存储的方式对外提供读服务，通常涉及频繁地生产消息和消费消息，这不属于典型的读多写少场景，因此读写分离方案在这个场景下并不太适合。

第三、Kafka副本机制使用的是异步消息拉取，因此存在leader和follower之间的不一致性。如果要采用读写分离，必然要处理副本lag引入的一致性问题，比如如何实现read-your-writes、如何保证单调读（monotonic reads）以及处理消息因果顺序颠倒的问题。相反地，如果不采用读写分离，所有客户端读写请求都只在Leader上处理也就没有这些问题了——当然最后全局消息顺序颠倒的问题在Kafka中依然存在，常见的解决办法是使用单分区，其他的方案还有version vector，但是目前Kafka没有提供。
